# This file is generated by AI
import os
import json
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import torch

# -----------------------------
# Utilities
# -----------------------------

def _read_vocab_txt(path: str) -> Dict[str, int]:
    """Read a BERT-style vocab.txt where each line is a token."""
    vocab: Dict[str, int] = {}
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            token = line.rstrip("\n")
            if token:
                vocab[token] = i
    return vocab


def _read_vocab_json(path: str) -> Dict[str, int]:
    """Read a vocab.json mapping token->id (common in some tokenizers)."""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, dict):
        raise ValueError("vocab.json must be a JSON object mapping token->id.")
    # Ensure int ids
    vocab: Dict[str, int] = {}
    for k, v in data.items():
        if not isinstance(v, int):
            raise ValueError(f"vocab.json has non-int id for token={k!r}")
        vocab[k] = v
    return vocab


def _whitespace_tokenize(text: str) -> List[str]:
    """Basic whitespace tokenization."""
    text = text.strip()
    if not text:
        return []
    return text.split()


def _basic_clean(text: str) -> str:
    """A very small subset of the cleaning done in real tokenizers."""
    # Collapse whitespace
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def _pad_to_max(
    ids: List[int],
    max_length: int,
    pad_id: int,
) -> Tuple[List[int], List[int]]:
    """Pad/truncate and return (padded_ids, attention_mask)."""
    if len(ids) > max_length:
        ids = ids[:max_length]
    attn = [1] * len(ids)
    if len(ids) < max_length:
        pad_len = max_length - len(ids)
        ids = ids + [pad_id] * pad_len
        attn = attn + [0] * pad_len
    return ids, attn


# -----------------------------
# Base Tokenizer
# -----------------------------

@dataclass
class TokenizerOutput:
    """A small output container similar to transformers BatchEncoding."""
    input_ids: torch.Tensor
    attention_mask: torch.Tensor

    def to(self, device: Union[str, torch.device]) -> "TokenizerOutput":
        self.input_ids = self.input_ids.to(device)
        self.attention_mask = self.attention_mask.to(device)
        return self


class BaseTokenizer:
    """
    Minimal tokenizer interface.

    Notes:
    - This is NOT a full HuggingFace tokenizer implementation.
    - We focus on the subset typically used in training/inference loops:
      encode, __call__, padding/truncation, returning torch tensors.
    """

    def __init__(
        self,
        vocab: Dict[str, int],
        unk_token: str = "[UNK]",
        pad_token: str = "[PAD]",
        cls_token: str = "[CLS]",
        sep_token: str = "[SEP]",
        mask_token: str = "[MASK]",
        add_special_tokens: bool = True,
    ):
        self.vocab = vocab
        self.inv_vocab = {i: t for t, i in vocab.items()}

        self.unk_token = unk_token
        self.pad_token = pad_token
        self.cls_token = cls_token
        self.sep_token = sep_token
        self.mask_token = mask_token
        self.add_special_tokens_default = add_special_tokens

        # Resolve ids (fallback to some sensible defaults if missing)
        self.unk_token_id = self.vocab.get(self.unk_token, 0)
        self.pad_token_id = self.vocab.get(self.pad_token, 0)
        self.cls_token_id = self.vocab.get(self.cls_token, self.unk_token_id)
        self.sep_token_id = self.vocab.get(self.sep_token, self.unk_token_id)
        self.mask_token_id = self.vocab.get(self.mask_token, self.unk_token_id)

    # ---- core methods you should override ----
    def tokenize(self, text: str) -> List[str]:
        raise NotImplementedError

    # ---- high-level API ----
    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:
        return [self.vocab.get(t, self.unk_token_id) for t in tokens]

    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:
        return [self.inv_vocab.get(i, self.unk_token) for i in ids]

    def build_inputs_with_special_tokens(self, token_ids: List[int]) -> List[int]:
        # Standard single-sequence format: [CLS] ... [SEP]
        return [self.cls_token_id] + token_ids + [self.sep_token_id]

    def encode(
        self,
        text: str,
        add_special_tokens: Optional[bool] = None,
        max_length: Optional[int] = None,
        truncation: bool = False,
    ) -> List[int]:
        add_special_tokens = self.add_special_tokens_default if add_special_tokens is None else add_special_tokens

        text = _basic_clean(text)
        tokens = self.tokenize(text)
        ids = self.convert_tokens_to_ids(tokens)

        if add_special_tokens:
            ids = self.build_inputs_with_special_tokens(ids)

        if max_length is not None and truncation and len(ids) > max_length:
            ids = ids[:max_length]

        return ids

    def __call__(
        self,
        text: Union[str, List[str]],
        padding: Union[bool, str] = False,
        truncation: bool = False,
        max_length: Optional[int] = None,
        return_tensors: str = "pt",
        add_special_tokens: Optional[bool] = None,
    ) -> TokenizerOutput:
        """
        Minimal call interface.

        Args:
            text: a single string or a list of strings
            padding: False / True / "max_length"
            truncation: whether to truncate when max_length is given
            max_length: pad/truncate length
            return_tensors: only supports "pt"
        """
        if return_tensors != "pt":
            raise ValueError("This minimal tokenizer only supports return_tensors='pt'.")

        if isinstance(text, str):
            texts = [text]
        else:
            texts = list(text)

        encoded: List[List[int]] = [
            self.encode(t, add_special_tokens=add_special_tokens, max_length=max_length, truncation=truncation)
            for t in texts
        ]

        # Determine padding length
        if padding is False:
            pad_to = None
        elif padding is True:
            pad_to = max(len(x) for x in encoded) if encoded else 0
        elif padding == "max_length":
            if max_length is None:
                raise ValueError("padding='max_length' requires max_length.")
            pad_to = max_length
        else:
            raise ValueError("padding must be False/True/'max_length'.")

        if pad_to is not None:
            input_ids_list: List[List[int]] = []
            attn_list: List[List[int]] = []
            for ids in encoded:
                padded_ids, attn = _pad_to_max(ids, pad_to, self.pad_token_id)
                input_ids_list.append(padded_ids)
                attn_list.append(attn)
            input_ids = torch.tensor(input_ids_list, dtype=torch.long)
            attention_mask = torch.tensor(attn_list, dtype=torch.long)
        else:
            # Ragged -> pad to max in batch anyway to make a tensor
            pad_to = max(len(x) for x in encoded) if encoded else 0
            input_ids_list, attn_list = [], []
            for ids in encoded:
                padded_ids, attn = _pad_to_max(ids, pad_to, self.pad_token_id)
                input_ids_list.append(padded_ids)
                attn_list.append(attn)
            input_ids = torch.tensor(input_ids_list, dtype=torch.long)
            attention_mask = torch.tensor(attn_list, dtype=torch.long)

        return TokenizerOutput(input_ids=input_ids, attention_mask=attention_mask)

    def batch_encode_plus(
        self,
        texts: List[str],
        padding: Union[bool, str] = True,
        truncation: bool = False,
        max_length: Optional[int] = None,
        return_tensors: str = "pt",
        add_special_tokens: Optional[bool] = None,
    ) -> TokenizerOutput:
        return self(
            texts,
            padding=padding,
            truncation=truncation,
            max_length=max_length,
            return_tensors=return_tensors,
            add_special_tokens=add_special_tokens,
        )

    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:
        tokens = self.convert_ids_to_tokens(ids)
        if skip_special_tokens:
            special = {self.pad_token, self.unk_token, self.cls_token, self.sep_token, self.mask_token}
            tokens = [t for t in tokens if t not in special]
        # Note: real tokenizers handle "##" wordpiece joining; we implement a tiny version here.
        out_words: List[str] = []
        for t in tokens:
            if t.startswith("##") and out_words:
                out_words[-1] = out_words[-1] + t[2:]
            else:
                out_words.append(t)
        return " ".join(out_words)


# -----------------------------
# WordPiece Tokenizer (BERT-like)
# -----------------------------

class WordPieceTokenizer(BaseTokenizer):
    """
    Minimal WordPiece tokenizer.

    Behavior:
    - Basic pre-tokenize: whitespace split
    - For each word, apply greedy longest-match-first subword segmentation
    - Use '##' prefix for continuation subwords (BERT convention)
    """

    def __init__(
        self,
        vocab: Dict[str, int],
        unk_token: str = "[UNK]",
        pad_token: str = "[PAD]",
        cls_token: str = "[CLS]",
        sep_token: str = "[SEP]",
        mask_token: str = "[MASK]",
        do_lower_case: bool = True,
        max_input_chars_per_word: int = 100,
        add_special_tokens: bool = True,
    ):
        super().__init__(
            vocab=vocab,
            unk_token=unk_token,
            pad_token=pad_token,
            cls_token=cls_token,
            sep_token=sep_token,
            mask_token=mask_token,
            add_special_tokens=add_special_tokens,
        )
        self.do_lower_case = do_lower_case
        self.max_input_chars_per_word = max_input_chars_per_word

    def tokenize(self, text: str) -> List[str]:
        if self.do_lower_case:
            text = text.lower()

        words = _whitespace_tokenize(text)
        output_tokens: List[str] = []

        for word in words:
            if len(word) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens: List[str] = []

            while start < len(word):
                end = len(word)
                cur_substr: Optional[str] = None

                while start < end:
                    piece = word[start:end]
                    if start > 0:
                        piece = "##" + piece
                    if piece in self.vocab:
                        cur_substr = piece
                        break
                    end -= 1

                if cur_substr is None:
                    is_bad = True
                    break

                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)

        return output_tokens


# -----------------------------
# Simplified "JSON vocab" tokenizer (demo-only)
# -----------------------------

class SimpleJsonVocabTokenizer(BaseTokenizer):
    """
    A very small tokenizer that:
    - splits by whitespace
    - looks up tokens directly in vocab.json
    This is NOT BPE. It exists only to demonstrate "AutoTokenizer dispatch".
    """

    def __init__(
        self,
        vocab: Dict[str, int],
        unk_token: str = "<unk>",
        pad_token: str = "<pad>",
        cls_token: str = "<s>",
        sep_token: str = "</s>",
        mask_token: str = "<mask>",
        add_special_tokens: bool = True,
    ):
        super().__init__(
            vocab=vocab,
            unk_token=unk_token,
            pad_token=pad_token,
            cls_token=cls_token,
            sep_token=sep_token,
            mask_token=mask_token,
            add_special_tokens=add_special_tokens,
        )

    def tokenize(self, text: str) -> List[str]:
        return _whitespace_tokenize(text)


# -----------------------------
# AutoTokenizer (minimal)
# -----------------------------

class AutoTokenizer:
    """
    Minimal stand-in for transformers.AutoTokenizer.

    Supported patterns:
    - from_pretrained(path):
        - if vocab.txt exists -> WordPieceTokenizer
        - elif vocab.json exists -> SimpleJsonVocabTokenizer (demo)
    """

    @staticmethod
    def from_pretrained(
        pretrained_model_name_or_path: str,
        **kwargs: Any,
    ) -> BaseTokenizer:
        """
        Create a tokenizer from a local directory.

        Args:
            pretrained_model_name_or_path: local folder path that contains tokenizer files.
            kwargs: passed to the underlying tokenizer class.
        """
        if not os.path.isdir(pretrained_model_name_or_path):
            raise ValueError(
                "This minimal AutoTokenizer only supports local directories. "
                f"Got: {pretrained_model_name_or_path!r}"
            )

        vocab_txt = os.path.join(pretrained_model_name_or_path, "vocab.txt")
        vocab_json = os.path.join(pretrained_model_name_or_path, "vocab.json")

        if os.path.isfile(vocab_txt):
            vocab = _read_vocab_txt(vocab_txt)
            # Allow overriding special tokens / lowercasing, etc.
            return WordPieceTokenizer(vocab=vocab, **kwargs)

        if os.path.isfile(vocab_json):
            vocab = _read_vocab_json(vocab_json)
            return SimpleJsonVocabTokenizer(vocab=vocab, **kwargs)

        raise FileNotFoundError(
            "Cannot infer tokenizer type. Expected one of:\n"
            f"  - {vocab_txt}\n"
            f"  - {vocab_json}\n"
            "In real transformers, AutoTokenizer can also use tokenizer_config.json, "
            "special_tokens_map.json, merges.txt, sentencepiece models, etc."
        )


# -----------------------------
# Example usage (for your sanity check)
# -----------------------------
if __name__ == "__main__":
    # Suppose you have a directory like:
    # ./bert_tokenizer/
    #   vocab.txt
    #
    # tokenizer = AutoTokenizer.from_pretrained("./bert_tokenizer", do_lower_case=True)
    #
    # out = tokenizer(["Hello world", "Hello there"], padding=True, return_tensors="pt")
    # print(out.input_ids.shape, out.attention_mask.shape)
    #
    # ids = tokenizer.encode("Hello world", truncation=True, max_length=8)
    # print(ids, tokenizer.decode(ids))
    pass
